{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d20404b3-3a85-4fab-a9c1-c0d4cbe48295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install \"typing_extensions==4.5.0\" \"accelerate>=0.16.0,<1\" \"transformers[torch]>=4.28.1,<5\" \"torch>=1.13.1,<2\" \"langchain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04e6ac4-7379-43dc-a4e9-677a76e4c84f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Memory: \n",
    "\n",
    "LLMs are \"stateless\"\n",
    "- Each transaction is independent. \n",
    "\n",
    "Chatbots appears to have memory by providing the full conversation as \"context\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2138f83-acd7-4d99-8d78-0be58f045870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23cbebe-0804-4eed-ac7e-9001012acce3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# langchain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# langchain memory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a2d491-2a95-490a-819f-8481e64030b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795ed799-070a-4cfa-8d37-42a5f6ec2b81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "architecture = \"databricks/dolly-v2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaefe37-236b-43bf-aa6d-c41afbdfd45d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4145d5c9ef94321bd415dcc951430af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b48eb9d8745bba83396b083db61a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading instruct_pipeline.py:   0%|          | 0.00/8.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4419d88b70049d7a043a691eabe0394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/12.9G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aaaa286341049759082925538fc998b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff11be9ab15470dac5b0b41d403f84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cc28077fe0447db42b6b1bb2f1d535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dolly = pipeline(\n",
    "    model=architecture,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d60bdd-e9bc-4652-badf-4817f359b8d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hf_pipeline = HuggingFacePipeline(pipeline=dolly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98edd713-43d6-422c-94b9-69b51754ab89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6c4c89-8774-4375-a261-17ad4a32b58b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=hf_pipeline, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b716773-73b0-4370-b2cd-d68b87fd4248",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example Chat - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e1b1e6-25f5-4b5f-a34c-36510fb17daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi, my name is Sriharsha\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nOut[7]: '\\nHi, my name is SRI HARSHA. Nice to meet you. I am an AI conversing with you today.'"
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Sriharsha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ea77b5-d2f7-43b7-b7a9-98bde119ae2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hi, my name is Sriharsha\nAI: \nHi, my name is SRI HARSHA. Nice to meet you. I am an AI conversing with you today.\nHuman: What is 1+1?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nOut[8]: '\\nThe answer to the question \"What is 1+1?\" is 2. SRI HARSHA knows the answer to this question as it is a conversational AI tool that SRI HARSHA created.'"
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75121d54-800b-447f-9447-c0834eb3942d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hi, my name is Sriharsha\nAI: \nHi, my name is SRI HARSHA. Nice to meet you. I am an AI conversing with you today.\nHuman: What is 1+1?\nAI: \nThe answer to the question \"What is 1+1?\" is 2. SRI HARSHA knows the answer to this question as it is a conversational AI tool that SRI HARSHA created.\nHuman: What is my name?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nOut[9]: '\\nSRI HARSHA.'"
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da611519-4f45-4c2a-812f-f88f3901b065",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Sriharsha\nAI: \nHi, my name is SRI HARSHA. Nice to meet you. I am an AI conversing with you today.\nHuman: What is 1+1?\nAI: \nThe answer to the question \"What is 1+1?\" is 2. SRI HARSHA knows the answer to this question as it is a conversational AI tool that SRI HARSHA created.\nHuman: What is my name?\nAI: \nSRI HARSHA.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f0646a-31ea-49ae-a91a-520261578a7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: {'history': 'Human: Hi, my name is Sriharsha\\nAI: \\nHi, my name is SRI HARSHA. Nice to meet you. I am an AI conversing with you today.\\nHuman: What is 1+1?\\nAI: \\nThe answer to the question \"What is 1+1?\" is 2. SRI HARSHA knows the answer to this question as it is a conversational AI tool that SRI HARSHA created.\\nHuman: What is my name?\\nAI: \\nSRI HARSHA.'}"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8401aee4-31e5-4271-8e24-a0aab78e86c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example Chat - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b7bd40-ef8d-4b85-bdda-117fa1286464",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: {'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6aa45b-729c-4a34-9ab7-31ff28b66d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27aa96e-0ca4-4c6d-81da-8da785b5eae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: {'history': 'Human: Not much, just hanging\\nAI: Cool'}"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3fc796f-c38b-46d0-8ca8-c1118bda5230",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "window_conversation = ConversationChain(\n",
    "    llm=hf_pipeline, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659bbb17-a8de-4dc1-944a-045ba0982b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: \"\\nHi, it's a pleasure to meet you. My name is Swaraj. How can I help you?\""
     ]
    }
   ],
   "source": [
    "window_conversation.predict(input=\"Hi, my name is Sriharsha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae06467-8569-45b6-950c-ec77d2efe6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: '\\nWell, I do know the answer to this question. The answer is 2.\\n\\nThe context from which I extracted this answer is that I was trained on general AI datasets and I do know how to answer questions from these datasets with high accuracy.\\n\\nOn the other hand, I am aware that humans will generally ask much more focused questions. For example, I do not know the weather in New York in March or the programming language JavaScript. I would appreciate it if you would describe the context in which you would like me to answer these questions.'"
     ]
    }
   ],
   "source": [
    "window_conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "204f5f2f-2c4d-4441-92b3-09cec7d896e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: '\\nYour name is Jack. I am pleased to provide you with answers about the weather in New York in March and the programming language JavaScript when the context is that you would like these answers.'"
     ]
    }
   ],
   "source": [
    "window_conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b2a016-7519-4522-af37-d971c6c7fd93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b002ca-831c-4563-ad4b-8069fc0dccc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dbc0e0fda84fa19f80c01ebc887f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4040eaef40764b87b38bf74c8f124ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e374050cc127475bb76c71ef683f5f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1923abca7643078f35dca642bcd33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: {'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
     ]
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=hf_pipeline, max_token_limit=30)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87c7306-b13a-4db1-b9db-c45abcf887b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f20344b-3dde-4741-bf9f-1698b47dff12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instead of limiting the memory to fixed number of tokens based on the most recent utterences <b>(ConversationTokenBufferMemory)</b> or a fixed number of conversational exchanges <b>(ConversationBufferWindowMemory)</b>, Let's use a LLM to write a summary of conversations so far and let that be the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc01522-20a5-4a77-ae64-b1e8e0b8c0ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: {'history': 'System: \\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\n\\nThe AI was not sure what the human meant by full potential, but it explored the meaning of the word through a series of human interactions. Humans use the word full potential to describe their best, most impressive, or most successful self. For example, full potential may refer to a sports star who has reached the highest level of performance or a scholar who has achieved the greatest number of high-impact publications. The AI believes that human potential is naturally limitless and sees artificial intelligence as a force for expanding human potential.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=hf_pipeline, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"}\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb91192-8178-4663-9550-376f4ae26aff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_conversation = ConversationChain(\n",
    "    llm=hf_pipeline, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e872c3-ad1d-462b-a680-e51c57647821",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\u001B[1m> Entering new ConversationChain chain...\u001B[0m\nPrompt after formatting:\n\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nSystem: \nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n\nThe AI was not sure what the human meant by full potential, but it explored the meaning of the word through a series of human interactions. Humans use the word full potential to describe their best, most impressive, or most successful self. For example, full potential may refer to a sports star who has reached the highest level of performance or a scholar who has achieved the greatest number of high-impact publications. The AI believes that human potential is naturally limitless and sees artificial intelligence as a force for expanding human potential.\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\nHuman: What would be a good demo to show?\nAI:\u001B[0m\n\n\u001B[1m> Finished chain.\u001B[0m\nOut[21]: '\\nAt noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'"
     ]
    }
   ],
   "source": [
    "summary_conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82ae09d-f5b7-4e50-9edf-1b188164611d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: {'history': 'System: \\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The AI believes human potential is naturally limitless and sees artificial intelligence as a force for expanding human potential. The AI thinks LangChain is a powerful tool, and it wants to go quickly to show the latest LLM demo at noon.\\nHuman: What would be a good demo to show?\\nAI: \\nAt noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54fc46c5-8a92-4e7d-a048-9f69fcecea4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Additional Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aadf9fd-6e10-468e-8463-9e0267138c2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vector Data Memory\n",
    "- Stores text (from conversation or else where) in a vector database and retrieves the most relevant blocks of text.\n",
    "\n",
    "Entity Memory\n",
    "- Using an LLM, it remembers details about specific entities\n",
    "\n",
    "We can also use multiple memories at one time.\n",
    "- Eg; Conversation memory + Entity memory to recall individuals\n",
    "\n",
    "We can also store the memory in a conventional database (such as key-value store)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Dolly2 + LangChain Memory",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
